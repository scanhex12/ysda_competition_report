# Отчет по соревнованию ML-1

### 0. Как побить бейзлайн

- Увеличиваем число итераций в lightgbm до 550

Также пробовал разделить данные и сделать бэггинг (```n_estimators = 3```) - работает хуже (вообще если разделять данные и последовательно подавать модели новую пачку, дообучая ее, то это работает значительно хуже)

Далее буду пытаться получить что-то полезное из исторических данных

### 1. Внедряем SVD
1. Получаем эмбеддинги users и requests с помощью svd (ноутбук svd_embeddings)
2. Добавляем полученные эмбеддинги как фичи для baseline

### 2. Делаем эмбеддинги поумнее - пишем DSSM

Обучение dssm лежит в файлике dssm_training.ipynb

Не уверен, что модель что-то выучила, потому что из-за огромного числа данных лосс не очень падает. Если посчитать среднее по окну, то получится примерно такая картинка лосса от "эпохи"(здесь эпоха - меньше, чем 1 проход по данным):

![image](https://user-images.githubusercontent.com/75157521/204125101-3b91147e-e4f7-4ee4-a19d-2d348978d1e4.png)

### 3. Добавляем lightfm

Обучаем на истории lightfm и добавляем его предсказания как фичу в модель

Это не увеличило скор, файлик lightgbm_lightfm.ipynb

### 4. Обучаем ALS

### 5. Попробуем кластеризовать эмбеддинги dssm

Будем использовать полученные кластера как фичи для изначальной модели

### 6. Попробуем кластеризовать эмбеддинги svd

Будем использовать полученные кластера как фичи для изначальной модели

Красивая картинка как выглядит кластеризация в случайной проекции:

![image](https://user-images.githubusercontent.com/75157521/204135665-7f6268d8-fad4-4159-b58d-b584b2abb57a.png)

