# Отчет по соревнованию ML-1

### 0. Как побить бейзлайн

- Увеличиваем число итераций в lightgbm до 550

Также пробовал разделить данные и сделать бэггинг (```n_estimators = 3```) - работает хуже (вообще если разделять данные и последовательно подавать модели новую пачку, дообучая ее, то это работает значительно хуже)

Далее буду пытаться получить что-то полезное из исторических данных

### 1. Внедряем SVD
1. Получаем эмбеддинги users и requests с помощью svd (ноутбук svd_embeddings)
2. Добавляем полученные эмбеддинги как фичи для baseline

Это дает небольшой прирост качества

Далее пойдут методы, которые может улучшат финальный скор, но нормально я это не проверил из-за нехватки ресурсов на beleriand в последние дни =(
Методы были протестированны на части датасета

### 2. Делаем эмбеддинги поумнее - пишем DSSM

Обучение dssm лежит в файлике dssm_training.ipynb

Не уверен, что модель что-то выучила, потому что из-за огромного числа данных лосс не очень падает. Если посчитать среднее по окну, то получится примерно такая картинка лосса от "эпохи"(здесь эпоха - меньше, чем 1 проход по данным):

![image](https://user-images.githubusercontent.com/75157521/204125101-3b91147e-e4f7-4ee4-a19d-2d348978d1e4.png)

### 3. Добавляем lightfm

Обучаем на истории lightfm и добавляем его предсказания как фичу в модель

Это не увеличило скор, файлик lightgbm_lightfm.ipynb


### 5. Попробуем кластеризовать эмбеддинги dssm

Будем использовать полученные кластера как фичи для изначальной модели

Красивая картинка как выглядит кластеризация в случайной проекции:

![image](https://user-images.githubusercontent.com/75157521/204136888-b928f824-b8d6-4e19-8471-d9494d5ff6d6.png)


### 6. Попробуем кластеризовать эмбеддинги svd

Будем использовать полученные кластера как фичи для изначальной модели

Опять же, красивая картинка как выглядит кластеризация в случайной проекции:

![image](https://user-images.githubusercontent.com/75157521/204135665-7f6268d8-fad4-4159-b58d-b584b2abb57a.png)

